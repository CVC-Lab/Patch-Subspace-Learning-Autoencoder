# -*- coding: utf-8 -*-
"""autoencoder&clustering2

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Z0BKHGjHmuh1fT6RfEkwJ2ntkAIJ-Wml

## Setup
"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 2.x
import tensorflow as tf
device_name = tf.test.gpu_device_name()
if device_name != '/device:GPU:0':
  raise SystemError('GPU device not found')
print('Found GPU at: {}'.format(device_name))

import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import math
import matplotlib.pyplot as plt
from scipy.stats import norm
import cv2
from keras.layers import Input, Dense, Lambda
from autoencoder import *

cwd = ''

"""## Load Dataset"""


validation_images = load_images(cwd + 'validation-r08-s-0000-of-0040.tfrecords')
images = load_images(cwd + 'train-r08-s-0000-of-0120.tfrecords')

"""## Generate Blur Images"""

blur_imgs = gen_noise(images)
val_blur_imgs = gen_noise(validation_images)


"""## Divide and Merge Images"""

block_size = 18
overlap = 4
block_per_image = 324
shape = (block_size, block_size, 3)

def gen_train_set(clear_imgs, blur_imgs, block_size):
  blur_images = []
  clear_images = []

  for i in range(len(clear_imgs)):
    blocks = divide_img(clear_imgs[i], block_size, overlap=overlap)
    for b in blocks:
      clear_images.append(b)
    blur_blocks = divide_img(blur_imgs[i], block_size, overlap=overlap)
    for bb in blur_blocks:
      blur_images.append(bb)
  return np.array(clear_images)/255, np.array(blur_images)/255

clear_images, blur_images = gen_train_set(images, blur_imgs, block_size=block_size)

"""## Build the encoder and decoder"""

latent_dim = 60
regularizer = keras.regularizers.l1_l2(0.01)

def build_encoder(latent_dim, shape):
  encoder_inputs = keras.Input(shape=shape)
  x = layers.Conv2D(16, 3, activation="relu", strides=1, padding="same", 
                    kernel_regularizer=regularizer)(encoder_inputs)
  x = layers.Conv2D(32, 3, activation="relu", strides=2, padding="same",
                    kernel_regularizer=regularizer)(x)
  x = layers.Conv2D(48, 3, activation="relu", strides=1, padding="same", 
                    kernel_regularizer=regularizer)(x)
  x = layers.Conv2D(72, 3, activation="relu", strides=2, padding="same", 
                    kernel_regularizer=regularizer)(x)
  x = layers.Conv2D(128, 3, activation="relu", strides=1, padding="same", 
                    kernel_regularizer=regularizer)(x)
  x = tf.keras.layers.BatchNormalization()(x)

  x = layers.Flatten()(x)
  x = layers.Dense(96, activation="relu", 
                   kernel_regularizer=regularizer)(x)
  x = layers.Dense(latent_dim, activation="relu", 
                   kernel_regularizer=regularizer)(x)
  encoder = keras.Model(encoder_inputs, x, name="encoder")
  return encoder


def build_decoder(latent_dim, shape, name):
  latent_inputs = keras.Input(shape=(latent_dim,))
  x = layers.Dense(shape[0] * shape[1] * 4, activation="relu",
                   kernel_regularizer=regularizer)(latent_inputs)
  x = layers.Reshape((shape[0]//6, shape[1]//6, 144))(x)
  x = layers.Conv2DTranspose(128, 3, activation="relu", strides=1, 
                             kernel_regularizer=regularizer, padding="same")(x)
  x = layers.Conv2DTranspose(72, 3, activation="relu", strides=2,
                             kernel_regularizer=regularizer, padding="same")(x)
  x = layers.Conv2DTranspose(48, 3, activation="relu", strides=1, 
                             kernel_regularizer=regularizer, padding="same")(x)
  x = layers.Conv2DTranspose(32, 3, activation="relu", strides=3, 
                             kernel_regularizer=regularizer, padding="same")(x)
  x = layers.Conv2DTranspose(16, 3, activation="relu", strides=1, 
                             kernel_regularizer=regularizer, padding="same")(x)
  decoder_outputs = layers.Conv2DTranspose(shape[2], 3, 
                                           activation="sigmoid", 
                                           kernel_regularizer=regularizer, 
                                           padding="same")(x)
  decoder = keras.Model(latent_inputs, decoder_outputs, name=name)
  return decoder


"""## Clustering"""

from typing import List
import numpy as np
import scipy as sp
from sklearn import cluster
from sklearn import decomposition
import sklearn

def cluster_latent(imgs: List[List[float]], num_cluster: int):
  n_clusters = num_cluster
  np.random.seed(0)
  
  inner_size = len(imgs[0])
  X = np.reshape(imgs,(-1, inner_size))

  k_means = cluster.KMeans(n_clusters=n_clusters, init='random', n_init= 8, random_state=0)
  k_means.fit(X)
  values = k_means.cluster_centers_.squeeze()
  labels = k_means.labels_
  centroids = k_means.cluster_centers_
  inertia = k_means.inertia_
  
  # y_kmeans = k_means.fit_predict(X)
  # pca =  decomposition.PCA(n_components=2)
  # X = pca.fit_transform(X)
  # colors = ['red', 'blue', 'green', 'yellow', 'black', 'purple']
  # for n in range(n_clusters):
  #   plt.scatter(X[y_kmeans==n, 0], X[y_kmeans==n, 1], s=5, c=colors[n], label ='Cluster'+str(n))
  # plt.show()

  return labels.tolist(), centroids.tolist(), inertia

def gen_clusters(imgs, labels, num_cluster):
  clusters = {}
  for label in labels:
    clusters[label] = []
  for i in range(len(labels)):
    clusters[labels[i]].append(imgs[i])
  return clusters

"""## Define the VAE as a `Model` with a custom `train_step`"""

class AutoEncoder(keras.Model):
    def __init__(self, encoder, decoder, **kwargs):
        super(AutoEncoder, self).__init__(**kwargs)
        self.encoder = encoder
        self.decoder = decoder

    def train_step(self, data):
        test = data[0][1]
        data = data[0][0]
        shape = (len(data[0]), len(data[0][0]))

        with tf.GradientTape() as tape:
            z = self.encoder(data)

            # reconstruct images
            reconstruction = self.decoder(z)
            # calculate loss
            reconstruction_loss = tf.reduce_mean(
                tf.keras.losses.MSE(test, reconstruction))
            reconstruction_loss *= shape[0] * shape[1]
            kl = tf.keras.losses.KLDivergence(
                reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE)
            kl_loss = kl(test, reconstruction)
            total_loss = reconstruction_loss + kl_loss
        
        grads = tape.gradient(total_loss, self.trainable_weights)
        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))
        return {
            "reconstruction_loss": reconstruction_loss,
        }

class AutoEncoder_P(keras.Model):
    def __init__(self, encoder, decoder, **kwargs):
        super(AutoEncoder_P, self).__init__(**kwargs)
        self.encoder = encoder
        self.trained_decoder = decoder

    def train_step(self, data):
        test = data[0][1]
        data = data[0][0]
        shape = (len(data[0]), len(data[0][0]))
        
        with tf.GradientTape() as tape:
            z = self.encoder(data)
            # reconstruct images
            reconstruction = self.trained_decoder(z)
            # calculate loss
            reconstruction_loss = tf.reduce_mean(
                tf.keras.losses.MSE(test, reconstruction))
            reconstruction_loss *= shape[0] * shape[1]
            kl = tf.keras.losses.KLDivergence(
                reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE)
            kl_loss = kl(test, reconstruction)
            total_loss = reconstruction_loss + kl_loss
        
        grads = tape.gradient(total_loss, self.trained_decoder.trainable_weights)
        self.optimizer.apply_gradients(zip(grads, self.trained_decoder.trainable_weights))
        return {
            "reconstruction_loss": reconstruction_loss,
        }

"""## Train the VAE"""

lr_schedule = keras.optimizers.schedules.ExponentialDecay(
    initial_learning_rate=0.001,
    decay_steps=1000,
    decay_rate=0.9)

encoder = build_encoder(latent_dim, shape)
decoder = build_decoder(latent_dim, shape,"decoder")
model = AutoEncoder(encoder, decoder)
model.compile(
    optimizer=keras.optimizers.Adam(learning_rate=lr_schedule))
model.fit((blur_images,clear_images), epochs=100, batch_size=128)

# clustering
batch = 10000
z = np.array(encoder(blur_images[:batch]))
for i in range(batch, len(blur_images), batch):
  z = np.concatenate([z, encoder(blur_images[i: i+batch])], axis=0)
labels, centroids, inertia = cluster_latent(z,2)
clusters = gen_clusters(blur_images, labels, 2)
label_clusters = gen_clusters(clear_images, labels, 2)

clus_1 = np.array(clusters[0])
clus_2 = np.array(clusters[1])
label_clus_1 = np.array(label_clusters[0])
label_clus_2 = np.array(label_clusters[1])

# train decoder 1
decoder1 = build_decoder(latent_dim, shape,"decoder1")
model_1 = AutoEncoder_P(encoder, decoder1)
model_1.compile(optimizer=keras.optimizers.Adam(learning_rate=lr_schedule))
model_1.fit((clus_1,label_clus_1), epochs=100, batch_size=128)

# train decoder 2
decoder2 = build_decoder(latent_dim, shape,"decoder2")
model_2 = AutoEncoder_P(encoder, decoder2)
model_2.compile(optimizer=keras.optimizers.Adam(learning_rate=lr_schedule))
model_2.fit((clus_2,label_clus_2), epochs=100, batch_size=128)

"""## Evaluation"""

def calc_dist(point_a, point_b):
  return np.linalg.norm(point_a - point_b)

def decode_images(z, centroids, decoders):
  decoded_images = []
  decode = []
  for decoder in decoders:
    decode.append(decoder.predict(z))
  for num in range(len(z)):
    i = z[num]
    clus = 0
    min_dist = 999999
    for c in range(len(centroids)):
      if calc_dist(i, centroids[c]) < min_dist:
        clus = c
        min_dist = calc_dist(i, centroids[c])
    decode_img = decode[clus][num]
    decoded_images.append(decode_img)
  return np.array(decoded_images)

def reconstruct_image(z, centroids, decoders, 
                      blocks_per_image=256, img_shape=(256,256), block_size=16):
  recons_images = []
  batch = 10000
  decoded_images = decode_images(z[:batch], centroids, decoders)
  for i in range(batch, len(z), batch):
    decoded_images = np.concatenate([decoded_images, decode_images(z[i:i+batch], centroids, decoders)], axis=0)
  
  for i in range(0, len(decoded_images), blocks_per_image):
    blocks = decoded_images[i: i+blocks_per_image]
    image = merge_img(blocks, img_shape[0], img_shape[1], block_size, overlap=overlap)
    recons_images.append(image)
  return np.array(recons_images)

test_images_clear, test_images_blur = gen_train_set(
    validation_images, val_blur_imgs, block_size=block_size)

batch = 10000
z = encoder.predict(test_images_blur[:batch])
for i in range(batch, len(test_images_blur), batch):
  z = np.concatenate([z, encoder.predict(test_images_blur[i: i+batch])], axis=0)

recons_images = reconstruct_image(z, centroids,
                                  decoders=[decoder1, decoder2],
                                  blocks_per_image=block_per_image,
                                  block_size=block_size)

comp_images = reconstruct_image(z, centroids, [decoder, decoder],
                                blocks_per_image=block_per_image,
                                block_size=block_size)
comp_images = (comp_images*255).astype('uint8')

test_images = []
for i in range(0, len(test_images_clear), block_per_image):
  test_images.append(merge_img(test_images_clear[i:i+block_per_image], 256, 256, block_size, overlap=overlap))
test_images = (np.array(test_images)*255).astype('uint8')

recons_images = (recons_images*255).astype('uint8')

"""## Quality Metric"""

cnt = 0
recons_psnr = []
comp_psnr = []
for i in range(len(recons_images)):
  recons_psnr.append(cv2.PSNR(recons_images[i], test_images[i]))
  comp_psnr.append(cv2.PSNR(comp_images[i], test_images[i]))
  if cv2.PSNR(recons_images[i], test_images[i]) > cv2.PSNR(comp_images[i], test_images[i]):
    cnt += 1
print('PSNR')
print(np.array(recons_psnr).mean(), np.array(comp_psnr).mean())
print(cnt/len(test_images))

from skimage.metrics import structural_similarity as ssim

cnt = 0
recons_ssim = []
comp_ssim = []
for i in range(len(recons_images)):
  recons_ssim.append(ssim(recons_images[i], test_images[i], multichannel=True))
  comp_ssim.append(ssim(comp_images[i], test_images[i], multichannel=True))
  if ssim(recons_images[i], test_images[i], multichannel=True) > ssim(comp_images[i], test_images[i], multichannel=True):
    cnt += 1
print('SSIM')
print(np.array(recons_ssim).mean(), np.array(comp_ssim).mean())
print(cnt/len(test_images))

import sewar
cnt = 0
recons_se = []
comp_se = []
for i in range(len(recons_images)):
  recons_se.append(sewar.full_ref.uqi(recons_images[i], test_images[i], ws=8))
  comp_se.append(sewar.full_ref.uqi(comp_images[i], test_images[i], ws=8))
  if sewar.full_ref.uqi(recons_images[i], test_images[i], 
                        ws=8) > sewar.full_ref.uqi(comp_images[i], test_images[i], ws=8):
    cnt += 1
print('UQI')
print(np.array(recons_se).mean(), np.array(comp_se).mean())
print(cnt/len(test_images))

