# -*- coding: utf-8 -*-
"""autoencoder&clustering2

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Z0BKHGjHmuh1fT6RfEkwJ2ntkAIJ-Wml

## Setup
"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 2.x
import tensorflow as tf
device_name = tf.test.gpu_device_name()
if device_name != '/device:GPU:0':
  raise SystemError('GPU device not found')
print('Found GPU at: {}'.format(device_name))

import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import math
import matplotlib.pyplot as plt
from scipy.stats import norm
import cv2
from keras.layers import Input, Dense, Lambda

cwd = ''

"""## Load Dataset"""

def load_images(file_path):
  raw_image_dataset = tf.data.TFRecordDataset(file_path)

  # Create a dictionary describing the features.
  image_feature_description = {
      'shape': tf.io.FixedLenFeature([3], tf.int64),
      'data': tf.io.FixedLenFeature([], tf.string),
      'label': tf.io.FixedLenFeature([1], tf.int64)
  }

  def _parse_image_function(example_proto):
    return tf.io.parse_single_example(example_proto, image_feature_description)
  parsed_image_dataset = raw_image_dataset.map(_parse_image_function)
  
  images = []
  for image_features in parsed_image_dataset:
    image_raw = image_features['data'].numpy()
    shape = image_features['shape'].numpy()
    img = tf.io.decode_raw(image_raw, tf.uint8)
    img = tf.reshape(img, shape).numpy()
    images.append(img)
  return np.array(images)


validation_images = load_images(cwd + 'validation-r08-s-0000-of-0040.tfrecords')
images = load_images(cwd + 'train-r08-s-0000-of-0120.tfrecords')

"""## Generate Blur Images"""

def gen_blur(images):
  topleft = 100
  size = 64
  blur_imgs = []
  images_copy = np.copy(images)
  for img in images_copy:
    b_img = img[topleft:topleft+size, topleft:topleft+size]
    b_img = cv2.blur(b_img,(20,20))
    for i in range(size):
      for j in range(size):
        img[i+topleft][j+topleft] = b_img[i][j]
    blur_imgs.append(img)
  blur_imgs = np.array(blur_imgs)
  return blur_imgs

'''
NOISE
'''
import random
def noisy(image, prob=0.05):
    output = np.zeros(image.shape,np.uint8)
    thres = 1 - prob 
    for i in range(image.shape[0]):
        for j in range(image.shape[1]):
            rdn = random.random()
            if rdn < prob:
                output[i][j] = 0
            elif rdn > thres:
                output[i][j] = 255
            else:
                output[i][j] = image[i][j]
    return output

def gen_noise(images, prob=0.05):
  noise = []
  for img in images:
    noise.append(noisy(img, prob))
  noise = np.array(noise).astype('uint8')
  return noise
'''
END
'''

blur_imgs = gen_noise(images)
val_blur_imgs = gen_noise(validation_images)


"""## Divide and Merge Images"""

block_size = 18
block_per_image = 324
shape = (block_size, block_size, 3)

def divide_img(img, block_size=18, num_block=18, overlap=4):
  height = len(img)
  width = len(img[0])
  if not (block_size*num_block - (num_block - 1)*overlap == height):
    print('Block size mismatch', 
          block_size*num_block - (num_block - 1)*overlap, height)
    return None
  size = block_size - overlap
  blocks = np.array([img[i:i+block_size, j:j+block_size] 
                     for j in range(0,width - overlap,size) 
                     for i in range(0,height - overlap,size)])
  return blocks

def merge_img(blocks, width=256, height=256, block_size=18, overlap=4):
  num_block_per_row = (width - overlap)//(block_size - overlap)
  num_block_per_col = (height - overlap)//(block_size - overlap)
  def get_row_block(row):
    row_block = blocks[row]
    for j in range(1, num_block_per_row):
      cur_row_block = row_block[:, :len(row_block[0]) - overlap]
      block1 = blocks[row+j*num_block_per_col]
      cur_block = block1[:, overlap:]
      lapping = row_block[:, len(row_block[0]) - overlap:]
      lapping1 = block1[:, :overlap]
      for k in range(0, overlap):
        lapping[:, k] *= 1 - (k+1)/(overlap+1)
        lapping1[:, k] *= (k+1)/(overlap+1)
      lap = lapping + lapping1
      row_block = np.concatenate([cur_row_block, lap, cur_block], axis=1)
    return row_block

  img = get_row_block(0)
  for i in range(1, num_block_per_col):
    cur_block = img[:len(img)-overlap]
    cur_row = get_row_block(i)
    lapping = img[len(img)-overlap:]
    lapping1 = cur_row[:overlap]
    cur_row = cur_row[overlap:]
    for k in range(0, overlap):
        lapping[k,:] *= 1 - (k+1)/(overlap+1)
        lapping1[k,:] *= (k+1)/(overlap+1)
    
    lap = lapping + lapping1
    img = np.concatenate([cur_block, lap, cur_row], axis=0)
  return img

def gen_train_set(clear_imgs, blur_imgs, block_size):
  blur_images = []
  clear_images = []

  for i in range(len(clear_imgs)):
    blocks = divide_img(clear_imgs[i], block_size)
    for b in blocks:
      clear_images.append(b)
    blur_blocks = divide_img(blur_imgs[i], block_size)
    for bb in blur_blocks:
      blur_images.append(bb)
  return np.array(clear_images)/255, np.array(blur_images)/255

clear_images, blur_images = gen_train_set(images, blur_imgs, block_size=block_size)

"""## Build the encoder and decoder"""

latent_dim = 60
regularizer = keras.regularizers.l1_l2(0.01)

def build_encoder(latent_dim, shape):
  encoder_inputs = keras.Input(shape=shape)
  x = layers.Conv2D(16, 3, activation="relu", strides=1, padding="same", 
                    kernel_regularizer=regularizer)(encoder_inputs)
  x = layers.Conv2D(32, 3, activation="relu", strides=2, padding="same",
                    kernel_regularizer=regularizer)(x)
  x = layers.Conv2D(48, 3, activation="relu", strides=1, padding="same", 
                    kernel_regularizer=regularizer)(x)
  x = layers.Conv2D(72, 3, activation="relu", strides=2, padding="same", 
                    kernel_regularizer=regularizer)(x)
  x = layers.Conv2D(128, 3, activation="relu", strides=1, padding="same", 
                    kernel_regularizer=regularizer)(x)
  x = tf.keras.layers.BatchNormalization()(x)

  x = layers.Flatten()(x)
  x = layers.Dense(96, activation="relu", 
                   kernel_regularizer=regularizer)(x)
  x = layers.Dense(latent_dim, activation="relu", 
                   kernel_regularizer=regularizer)(x)
  encoder = keras.Model(encoder_inputs, x, name="encoder")
  return encoder


def build_decoder(latent_dim, shape, name):
  latent_inputs = keras.Input(shape=(latent_dim,))
  x = layers.Dense(shape[0] * shape[1] * 4, activation="relu",
                   kernel_regularizer=regularizer)(latent_inputs)
  x = layers.Reshape((shape[0]//6, shape[1]//6, 144))(x)
  x = layers.Conv2DTranspose(128, 3, activation="relu", strides=1, 
                             kernel_regularizer=regularizer, padding="same")(x)
  x = layers.Conv2DTranspose(72, 3, activation="relu", strides=2,
                             kernel_regularizer=regularizer, padding="same")(x)
  x = layers.Conv2DTranspose(48, 3, activation="relu", strides=1, 
                             kernel_regularizer=regularizer, padding="same")(x)
  x = layers.Conv2DTranspose(32, 3, activation="relu", strides=3, 
                             kernel_regularizer=regularizer, padding="same")(x)
  x = layers.Conv2DTranspose(16, 3, activation="relu", strides=1, 
                             kernel_regularizer=regularizer, padding="same")(x)
  decoder_outputs = layers.Conv2DTranspose(shape[2], 3, 
                                           activation="sigmoid", 
                                           kernel_regularizer=regularizer, 
                                           padding="same")(x)
  decoder = keras.Model(latent_inputs, decoder_outputs, name=name)
  return decoder


"""## Clustering"""

from typing import List
import numpy as np
import scipy as sp
from sklearn import cluster
from sklearn import decomposition
import sklearn

def cluster_latent(imgs: List[List[float]], num_cluster: int):
  n_clusters = num_cluster
  np.random.seed(0)
  
  inner_size = len(imgs[0])
  X = np.reshape(imgs,(-1, inner_size))

  k_means = cluster.KMeans(n_clusters=n_clusters, init='random', n_init= 8, random_state=0)
  k_means.fit(X)
  values = k_means.cluster_centers_.squeeze()
  labels = k_means.labels_
  centroids = k_means.cluster_centers_
  inertia = k_means.inertia_
  
  # y_kmeans = k_means.fit_predict(X)
  # pca =  decomposition.PCA(n_components=2)
  # X = pca.fit_transform(X)
  # colors = ['red', 'blue', 'green', 'yellow', 'black', 'purple']
  # for n in range(n_clusters):
  #   plt.scatter(X[y_kmeans==n, 0], X[y_kmeans==n, 1], s=5, c=colors[n], label ='Cluster'+str(n))
  # plt.show()

  return labels.tolist(), centroids.tolist(), inertia

def gen_clusters(imgs, labels, num_cluster):
  clusters = {}
  for label in labels:
    clusters[label] = []
  for i in range(len(labels)):
    clusters[labels[i]].append(imgs[i])
  return clusters

"""## Define the VAE as a `Model` with a custom `train_step`"""

class AutoEncoder(keras.Model):
    def __init__(self, encoder, decoder, **kwargs):
        super(AutoEncoder, self).__init__(**kwargs)
        self.encoder = encoder
        self.decoder = decoder

    def train_step(self, data):
        test = data[0][1]
        data = data[0][0]
        shape = (len(data[0]), len(data[0][0]))

        with tf.GradientTape() as tape:
            z = self.encoder(data)

            # reconstruct images
            reconstruction = self.decoder(z)
            # calculate loss
            reconstruction_loss = tf.reduce_mean(
                tf.keras.losses.MSE(test, reconstruction))
            reconstruction_loss *= shape[0] * shape[1]
            kl = tf.keras.losses.KLDivergence(
                reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE)
            kl_loss = kl(test, reconstruction)
            total_loss = reconstruction_loss + kl_loss
        
        grads = tape.gradient(total_loss, self.trainable_weights)
        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))
        return {
            "reconstruction_loss": reconstruction_loss,
        }

class AutoEncoder_P(keras.Model):
    def __init__(self, encoder, decoder, **kwargs):
        super(AutoEncoder_P, self).__init__(**kwargs)
        self.encoder = encoder
        self.trained_decoder = decoder

    def train_step(self, data):
        test = data[0][1]
        data = data[0][0]
        shape = (len(data[0]), len(data[0][0]))
        
        with tf.GradientTape() as tape:
            z = self.encoder(data)
            # reconstruct images
            reconstruction = self.trained_decoder(z)
            # calculate loss
            reconstruction_loss = tf.reduce_mean(
                tf.keras.losses.MSE(test, reconstruction))
            reconstruction_loss *= shape[0] * shape[1]
            kl = tf.keras.losses.KLDivergence(
                reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE)
            kl_loss = kl(test, reconstruction)
            total_loss = reconstruction_loss + kl_loss
        
        grads = tape.gradient(total_loss, self.trained_decoder.trainable_weights)
        self.optimizer.apply_gradients(zip(grads, self.trained_decoder.trainable_weights))
        return {
            "reconstruction_loss": reconstruction_loss,
        }

"""## Train the VAE"""

lr_schedule = keras.optimizers.schedules.ExponentialDecay(
    initial_learning_rate=0.001,
    decay_steps=1000,
    decay_rate=0.9)

encoder = build_encoder(latent_dim, shape)
decoder = build_decoder(latent_dim, shape,"decoder")
model = AutoEncoder(encoder, decoder)
model.compile(
    optimizer=keras.optimizers.Adam(learning_rate=lr_schedule))
model.fit((blur_images,clear_images), epochs=100, batch_size=128)

# clustering
batch = 10000
z = np.array(encoder(blur_images[:batch]))
for i in range(batch, len(blur_images), batch):
  z = np.concatenate([z, encoder(blur_images[i: i+batch])], axis=0)
labels, centroids, inertia = cluster_latent(z,2)
clusters = gen_clusters(blur_images, labels, 2)
label_clusters = gen_clusters(clear_images, labels, 2)

clus_1 = np.array(clusters[0])
clus_2 = np.array(clusters[1])
label_clus_1 = np.array(label_clusters[0])
label_clus_2 = np.array(label_clusters[1])

# train decoder 1
decoder1 = build_decoder(latent_dim, shape,"decoder1")
model_1 = AutoEncoder_P(encoder, decoder1)
model_1.compile(optimizer=keras.optimizers.Adam(learning_rate=lr_schedule))
model_1.fit((clus_1,label_clus_1), epochs=100, batch_size=128)

# train decoder 2
decoder2 = build_decoder(latent_dim, shape,"decoder2")
model_2 = AutoEncoder_P(encoder, decoder2)
model_2.compile(optimizer=keras.optimizers.Adam(learning_rate=lr_schedule))
model_2.fit((clus_2,label_clus_2), epochs=100, batch_size=128)

"""## Evaluation"""

def calc_dist(point_a, point_b):
  return np.linalg.norm(point_a - point_b)

def decode_images(z, centroids, decoders):
  decoded_images = []
  decode = []
  for decoder in decoders:
    decode.append(decoder.predict(z))
  for num in range(len(z)):
    i = z[num]
    clus = 0
    min_dist = 999999
    for c in range(len(centroids)):
      if calc_dist(i, centroids[c]) < min_dist:
        clus = c
        min_dist = calc_dist(i, centroids[c])
    decode_img = decode[clus][num]
    decoded_images.append(decode_img)
  return np.array(decoded_images)

def reconstruct_image(z, centroids, decoders, 
                      blocks_per_image=256, img_shape=(256,256), block_size=16):
  recons_images = []
  batch = 10000
  decoded_images = decode_images(z[:batch], centroids, decoders)
  for i in range(batch, len(z), batch):
      decoded_images = np.concatenate([decoded_images, decode_images(z[i:i+batch], centroids, decoders)], axis=0)
  
  for i in range(0, len(decoded_images), blocks_per_image):
    blocks = decoded_images[i: i+blocks_per_image]
    image = merge_img(blocks, img_shape[0], img_shape[1], block_size)
    recons_images.append(image)
  return np.array(recons_images)

test_images_clear, test_images_blur = gen_train_set(
    validation_images, val_blur_imgs, block_size=block_size)

z = encoder.predict(test_images_blur)
batch = 10000
z = encoder.predict(test_images_blur[:batch])
for i in range(batch, len(test_images_blur), batch):
    z = np.concatenate([z, encoder.predict(test_images_blur[i: i+batch])], axis=0)

recons_images = reconstruct_image(z, centroids,
                                  decoders=[decoder1, decoder2],
                                  blocks_per_image=block_per_image,
                                  block_size=block_size)

comp_images = reconstruct_image(z, centroids, [decoder, decoder],
                                blocks_per_image=block_per_image,
                                block_size=block_size)
comp_images = (comp_images*255).astype('uint8')

test_images = []
for i in range(0, len(test_images_clear), block_per_image):
  test_images.append(merge_img(test_images_clear[i:i+block_per_image], 256, 256, block_size))
test_images = (np.array(test_images)*255).astype('uint8')

recons_images = (recons_images*255).astype('uint8')

"""## Quality Metric"""

cnt = 0
recons_psnr = []
comp_psnr = []
for i in range(len(recons_images)):
  recons_psnr.append(cv2.PSNR(recons_images[i], test_images[i]))
  comp_psnr.append(cv2.PSNR(comp_images[i], test_images[i]))
  if cv2.PSNR(recons_images[i], test_images[i]) > cv2.PSNR(comp_images[i], test_images[i]):
    cnt += 1
print('PSNR')
print(np.array(recons_psnr).mean(), np.array(comp_psnr).mean())
print(cnt/len(test_images))

from skimage.metrics import structural_similarity as ssim

cnt = 0
recons_ssim = []
comp_ssim = []
for i in range(len(recons_images)):
  recons_ssim.append(ssim(recons_images[i], test_images[i], multichannel=True))
  comp_ssim.append(ssim(comp_images[i], test_images[i], multichannel=True))
  if ssim(recons_images[i], test_images[i], multichannel=True) > ssim(comp_images[i], test_images[i], multichannel=True):
    cnt += 1
print('SSIM')
print(np.array(recons_ssim).mean(), np.array(comp_ssim).mean())
print(cnt/len(test_images))

import sewar
cnt = 0
recons_se = []
comp_se = []
for i in range(len(recons_images)):
  recons_se.append(sewar.full_ref.uqi(recons_images[i], test_images[i], ws=8))
  comp_se.append(sewar.full_ref.uqi(comp_images[i], test_images[i], ws=8))
  if sewar.full_ref.uqi(recons_images[i], test_images[i], 
                        ws=8) > sewar.full_ref.uqi(comp_images[i], test_images[i], ws=8):
    cnt += 1
print('UQI')
print(np.array(recons_se).mean(), np.array(comp_se).mean())
print(cnt/len(test_images))

